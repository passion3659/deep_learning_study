{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lec06-1-fundamentals-of-machine-learning.ipynb","provenance":[{"file_id":"1TaAnOrSJr0rPN_5VBPj78fUr6yuZglUl","timestamp":1554096971868},{"file_id":"1-GA4RXQPyAGWkvdBMBBGzOj61nUDNQes","timestamp":1552904168896}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dbGDwo4AQFi-"},"source":["\n","## Four branches of machine learning\n","  "]},{"cell_type":"markdown","metadata":{"id":"agQMvaVead7Q"},"source":["* We have seen three specific types of machine learning problems: binary classification, multiclass classification, and scalar regression.\n","* All three are instances of *supervised learning*.\n","* Machine learning algorithms generally fall into four broad categories, described in the below."]},{"cell_type":"markdown","metadata":{"id":"ES-0w15WqfsX"},"source":["> ### Supervised learning"]},{"cell_type":"markdown","metadata":{"id":"q4WYAQ1faDPG"},"source":["* The most common case\n","\n","* It consists of learning to map input data to known targets (also called *annotations*), given a set of examples (often annotated by humans).\n","\n","* Generally, almost all applications of deep learning that are in the spotlight these days belong in this category.\n","  * E.g., optical character recognition, speech recognition, image classification, and language translation\n","\n","* Although supervised learning mostly consists of classification and regression, there are more variants as well.\n","  * Sequence generation: Given a picture, predict a caption describing it.\n","      \n","    <img src=\"https://drive.google.com/uc?id=13W2tZ27kqVmNEOa8ygs17jjVMAGebWKO\" width=\"800\">\n","    \n","    *https://cs.stanford.edu/people/karpathy/sfmltalk.pdf*\n","  \n","  * Syntax tree prediction: Given a sentence, predict its decomposition into a syntax tree.\n","  \n","  * Object detection: Given a picture, draw a bounding box around certain objects inside the picture.\n","  \n","    <img src=\"https://drive.google.com/uc?id=13WuTOydqvbLAuQ15C5P0Gj68srmAZ9NF\" width=\"800\">\n","    \n","    *https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088*\n","    \n","  * Object segmentation: Given a picture, draw a pixe-level mask on a specific object.\n","  \n","    <img src=\"https://drive.google.com/uc?id=13_-4dEBvtg9xRD-sqM1eyaLMNUz4Gvee\" width=\"800\">\n","    \n","    *https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359* \n","  \n","  "]},{"cell_type":"markdown","metadata":{"id":"cVk9ih1deqCg"},"source":["> ### Unsupervised learning"]},{"cell_type":"markdown","metadata":{"id":"_I1VIiW0eyc_"},"source":["* Finding interesting transformations of the input data without the help of any targets for the purpose of:\n","  * data visualization\n","  * data compression\n","  * data denoising\n","  * to better understand the correlations present in the data at hand\n","  \n","* *Dimensionality reduction* and *clustering* are well-known categories of unsupervised learning.\n"]},{"cell_type":"markdown","metadata":{"id":"G-OMgmGzQxoN"},"source":["> ### Self-supervised Learning"]},{"cell_type":"markdown","source":["전통적인 딥러닝은 라벨링을 다 해야하는데 2세대의 딥러닝은 라벨링된거를 하나만 보고 스스로 특징들을 비슷한거를 생각해서 다른것들도 알아서 코끼리라고 라벨링한다고 생각하면 된다는것인거같은데.. 좀 더 나아가서 직접 알아서 라벨링을 해서 사람이 라벨링 즉 annotation 할 필요가 없다."],"metadata":{"id":"sg0Wnxk7AkbW"}},{"cell_type":"markdown","metadata":{"id":"pLLGWawfQ01E"},"source":["* A specific instance of supervised learning\n","* Self-supervised learning is supervised learning without human-annotated labels.\n","* There are still labels involved, but they are generated from the input data.\n","* Examples\n","  * autoencoders where the generated targets are the input\n","  * predicting the next frame in a video, given past frames\n","  * predicting the next word in a text, given previous words\n"]},{"cell_type":"markdown","metadata":{"id":"5xaClvoCfU5u"},"source":["> ### Reinforcement learning"]},{"cell_type":"markdown","source":["수업시간에 다루지는 않는데 machine learning에서 매우 중요한 부분이다<br>\n","장기적인 것을 다루는 느낌?"],"metadata":{"id":"U-npPqE3B0-O"}},{"cell_type":"markdown","metadata":{"id":"EZP0gEIigT19"},"source":["* Reinforcement learning started to get a lot of attention after Google DeepMind successfully applied it to learning to play Atari games.\n","  * https://www.youtube.com/watch?v=V1eYniJ0Rnk&vl=en\n","  \n","* In RL, an *agent* receives information about its environment and learns to choose actions that will maximize some reward.\n","  * For example, a neural network that \"looks\" at a video-game screen and outputs game actions in order to maximize its score can be trained via RL.\n","  \n","* It can be applied to large range of real-world applications:\n","  * self-driving cars, robotics, resource management, education, and so on."]},{"cell_type":"markdown","metadata":{"id":"H4fdXO5ciazU"},"source":["## Evaluating machine-learning models"]},{"cell_type":"markdown","metadata":{"id":"KUB7wbiji94k"},"source":["* In the previous examples, we split the data into a training set, a validation set, and a test set.\n","\n","* In machine learning, the goal is to achieve models that *generalize* - that perform well on never-before-seen data - and overfitting is the central obstacle.\n","\n","* Here, we will focus on how to measure generalization: how to evaluate machine-learning models"]},{"cell_type":"markdown","metadata":{"id":"iUAplk2ZkPba"},"source":["> ### Training, validation, and test sets"]},{"cell_type":"markdown","source":["딥러닝은 보통 hold-out데이터를 쓴다"],"metadata":{"id":"Atv8rhLZKAOE"}},{"cell_type":"markdown","source":["valid을 하는 이유 <br>\n","그냥 최적의 hyperparameter를 찾기 위해서이다<br>\n","test는 loss function후 그냥 자동으로 최적화되는것을 알아보기 위한 단계이다"],"metadata":{"id":"YShmfudaF7Nd"}},{"cell_type":"markdown","metadata":{"id":"XzT3v31gkR19"},"source":["* Splitting the available data into three sets: training, validation, and test.\n","  * We train on the training data and evaluate our model on the validation data.\n","  * Once the model is ready, we test it one final time on the test data.\n","  \n","* Why not have just two sets: a training set and a test set?\n","\n","* The reason is that developing a model always involves tuning its configuration.\n","  * For example, choosing the number of layers or the size of the layers\n","    * They are called the *hyperparameters* of the model, to distinguish them from the *parameters*, which are the network's weights.\n","  * We do this tuning by using the performance of the model on the validation data.\n","  * This tuning is a form of *learning*: a search for a good configuration in some parameter space.\n","  * As a result, tuning the configuration of the model can quickly result in *overfitting to the validation set*, even though your model is never directly trained on it.\n","  \n","* Central to this phenomenon is the notion of *information leak*.\n","  * Every time you tune a hyperparameter of your model based on the model's performance on the validation set, some information about the validation data leaks into the model.\n","  \n","* A model that performs artificially well on the validation set does not guarantee similar performance on the test set."]},{"cell_type":"markdown","metadata":{"id":"RB9NqVYDma61"},"source":["* **Simple hold-out validation**\n"," \n","    <img src=\"https://drive.google.com/uc?id=13fgXDda-SVywl2lhsyIhpdCs7wbMFqdK\" width=\"800\">\n","  \n","  * The simplest evaluation protocol\n","    * If little data is available, then the validation and test sets may contain too few samples to be statistically representative of the data at hand.\n","      * It is easy to observe: try different random shuffling rounds of the data\n","  \n","  * Code example\n","  \n","  ```python\n","    # hold-out validation\n","    num_validation_samples = 10000\n","\n","    np.random.shuffle(data)\n","\n","    validation_data = data[:num_validation_samples]\n","    data = data[num_validation_samples:]\n","\n","    training_data = data[:]\n","\n","    model = get_model()\n","    model.train(training_data)\n","    validation_score = model.evaluate(validation_data)\n","\n","    # At this point you can tune your model!\n","\n","    model = get_model()\n","    model.train(np.concatenate([training_data, validation_data]))\n","\n","    test_score = model.evaluate(test_data)\n","  ```"]},{"cell_type":"markdown","metadata":{"id":"KCxzMzNgqd20"},"source":["* **k-fold validation**\n","\n","  * Split the data into *k* partitions of equal size.\n","  * For each partition *i*, train a model on the remaining *k-1* partitions, and evaluate it on partition *i*.\n","  * Then, the average of the *k* scores is obtained as the final score.\n","  \n","    <img src=\"https://drive.google.com/uc?id=13glqM37H-fU8KZhZudfo35XTi-floJWi\" width=\"800\">\n","    \n","    \n","  * Code example\n","  \n","  ```python\n","    k = 4\n","    num_validation_samples = len(data) // k\n","\n","    np.random.shuffle(data)\n","\n","    validation_scores = []\n","    for fold in range(k):\n","      validation_data = data[num_validation_samples*fold : num_validation_samples*(fold+1)]\n","      training_data = data[:num_validation_samples*fold] + data[num_validation_samples*(fold+1):]\n","\n","      model = get_model()\n","      model.train(training_data)\n","      validation_score = model.evaluate(validation_data)\n","      validation_scores.append(validation_score)\n","\n","    validation_score = np.average(validation_scores)\n","\n","    model = get_model()\n","    model.train(data)\n","    test_score = model.evaluate(test_data)\n","  ```"]},{"cell_type":"markdown","metadata":{"id":"qMI5L1YtsaGk"},"source":["* **Iterated k-fold validation with shuffling** \n","  \n","  * Applying k-fold validation multiple times, shuffling the data every time before splitting it *k* ways\n","  * The final score is the average of the scores obtained at each run of k-fold validation."]},{"cell_type":"markdown","source":["데이터가 극단적으로 적을때 쓰는 방법<br>\n","k-fold validation을 여러번하는것이다"],"metadata":{"id":"9XxLJm8ZJWd8"}},{"cell_type":"markdown","metadata":{"id":"BjfTs_kOsyXr"},"source":["> ### Things to keep in mind"]},{"cell_type":"markdown","source":["데이터가 잘 대표하고 있는가 --> 셔플해서 뽑는거 <br>\n","시계열데이터는 섞으면 큰일남  알아서 잘 대표하게 해라<br>\n","중복도 잘 제거해야한다"],"metadata":{"id":"8bKslY0lKLmj"}},{"cell_type":"markdown","metadata":{"id":"4MKpqVygs53m"},"source":["* Data representativeness\n","  * What if you sort the data according to their classes?\n","  * *random shuffling* is usually used before splitting it.\n","  \n","* The arrow of time\n","  * If you are trying to predict the future given the past, you should *not* randomly shuffle the data before splitting it.\n","  \n","* Redundancy in your data\n","  * If some data points in your data appear twice, then the performance might be over-estimated.\n","  * Make sure your training set and validation set are disjoint."]},{"cell_type":"markdown","metadata":{"id":"860bvWeAtzbR"},"source":["## Data preprocessing, feature engineering, and feature learning"]},{"cell_type":"markdown","metadata":{"id":"7KMs7J6sQIvZ"},"source":["* How do we prepare the input data and targets before feeding them into a neural network?\n","* Many data-preprocessing and feature-engineering techniques are domain specific."]},{"cell_type":"markdown","metadata":{"id":"HW1IGwzcSmcW"},"source":["> ### Data preprocessing for neural networks"]},{"cell_type":"markdown","metadata":{"id":"UdvI-tAFz4WH"},"source":["* Vectorization\n","  * (input, target) --> tensors of floating-point data\n","  \n","* Value normalization\n","  * Normalize each feature independently so that it had a standard deviation of 1 and a mean of 0.\n"," \n","* Handling missing values\n","  * With neural networks, it is safe to input missing values as 0."]},{"cell_type":"markdown","metadata":{"id":"qENSpTYO3BdX"},"source":["> ### Feature engineering"]},{"cell_type":"markdown","source":["task하기 위해 정보를 추출하는 방법<br>\n","아마 딥러닝은 별로 필요없다"],"metadata":{"id":"7q2rLu8aMebU"}},{"cell_type":"markdown","metadata":{"id":"p5BM89Xv3DYg"},"source":["* The process of using our own knowledge about the data to make the algorithm work better by applying hardcoded (nonlearned) transformations to the data.\n","\n","* Reading the time on a clock\n","\n","     <img src=\"https://drive.google.com/uc?id=15Lw3owKBH2UgnwEUz-o-8AX2J9L2-V6H\" width=\"500\">\n","\n","* Before deep learning, feature engineering used to be critical.\n","  * Because classical shallow algorithms did not have hypothesis spaces rich enough to learn useful features by themselves.\n","  * E.g., MNIST --> the number of loops, the height of each digit, a histogram of pixel values, etc.\n","  \n","* Modern deep learning removes the need for most feature engineering.\n","  * Because neural networks are capable of automatically extracting useful features from raw data.\n","  \n","* However, this is still important for two reasons:\n","  * Good features allow us to solve problems more elegantly while using fewer resources.\n","  * Good features let us solve a problem with far less data."]},{"cell_type":"markdown","source":["하지만 그래도 feature는 중요하다 feature특징을 그냥 알아내면 딥러닝까지 갈 필요가 없다"],"metadata":{"id":"FJU6m5SRM7q1"}},{"cell_type":"markdown","metadata":{"id":"itjhpHsK3MeT"},"source":["## Overfitting and underfitting"]},{"cell_type":"markdown","metadata":{"id":"U8hYZ7AB5crG"},"source":["* The fundamental issue in machine learning is the tension between optimization and generalization.\n","  * *Optimization* refers to the process of adjusting a model to get the best performance on the training data.\n","  * *Generalization* refers to how well the trained model performs on data it has never seen before.\n","  * The goal is to get good generalization, but we can only adjust the model based on the training data.\n","  \n","* At the beginning of training, optimization and generalization are correlated.\n","  * The lower the loss on training data, the lower on test data.\n","  * While this is happening, the model is said to be *underfit*.\n","  \n","* After a certain number of iterations, generalization stops improving.\n","  * The model is starting to *overfit*.\n","  \n","* To prevent overfitting, the best solution is to get *more training data*.\n","\n","* When that isn't possible, the next-best solution is to modulate \n","  * the quantity of information that the model is allowed to store,\n","  * to add constraints on what information it's allowed to store.\n","  * The process of fighting overfitting this way is called *regularization*."]},{"cell_type":"markdown","metadata":{"id":"WFlTWhVG7LsV"},"source":["> ### Reducing the network's size"]},{"cell_type":"markdown","source":["overfitting을 막기 위한 방법은 모델의 complexity를 줄이는것이다"],"metadata":{"id":"5GKlFR3uPMa8"}},{"cell_type":"markdown","metadata":{"id":"p-qUzKeA7PQ8"},"source":["* The simplest way to prevent overfitting is to reduce the size of the model: the number of learnable parameters in the model.\n","\n","* In deep learning, the number of learnable parameters in a model is often referred to as the model's *capacity*.\n","\n","* There is a compromise to be found between *too much capacity* and *not enough capacity*.\n","\n","* Unfortunately, there is no magical formula to determine the right number of layers or the right size for each layer.\n","\n","* Let's revisit the movie-review classification network.\n","  * The original model\n","  \n","    ```python\n","      from tensorflow.keras import models \n","      from tensorflow.keras import layers\n","\n","      model = models.Sequential() \n","      model.add(layers.Dense(16, activation='relu', input_shape=(10000,))) \n","      model.add(layers.Dense(16, activation='relu')) \n","      model.add(layers.Dense(1, activation='sigmoid'))\n","    ```\n","  \n","  * Smaller network (low capacity)\n","  \n","    ```python\n","      model = models.Sequential() \n","      model.add(layers.Dense(4, activation='relu', input_shape=(10000,))) \n","      model.add(layers.Dense(4, activation='relu')) \n","      model.add(layers.Dense(1, activation='sigmoid'))\n","    ```\n","  \n","  * A comparison of the validation losses of the original network and the smaller network\n","  \n","    <img src=\"https://drive.google.com/uc?id=15O-efTcA7xWc77HuvniLf9zNGgdLAXKL\" width=\"500\">\n","  \n","  * Bigger model (high capacity)\n","  \n","    ```python\n","      model = models.Sequential() \n","      model.add(layers.Dense(512, activation='relu', input_shape=(10000,))) \n","      model.add(layers.Dense(512, activation='relu')) \n","      model.add(layers.Dense(1, activation='sigmoid'))\n","    ```\n","  \n","  * A comparison between the original network and the bigger network\n","  \n","    <img src=\"https://drive.google.com/uc?id=15OWpylDoU1zY4dAbrsWUUwXHP_NHFTuM\" width=\"500\">\n","\n","    <img src=\"https://drive.google.com/uc?id=15eF8nKy2OLIKPTukUSeevTRbeOG_BEUB\" width=\"500\">\n","  "]},{"cell_type":"markdown","metadata":{"id":"J18mqGsI9y7m"},"source":["> ### Adding weight regularization"]},{"cell_type":"markdown","source":["모델의 hyperparamet를 줄여서 과적합을 막을수도 있지만 regularization으로도 가능하다<br>\n","일단 기본적으로 앞에 있는 parameter 즉 계수들이 아주 크면 그래프가 overfitting되는 특성이 있다.<br>\n","그래서 그 앞의 계수들에 제한을 두면 overfitting을 막을수가잇다.<br>\n","l1 regulrazation은 그 계수들의 절대값 값을 최소화시키는 방법<br>\n","l2는 제곱의 합을 작게하는 방법이다"],"metadata":{"id":"ETLD7q9jQqN0"}},{"cell_type":"markdown","metadata":{"id":"u6jRe-JF99zk"},"source":["* The principle of *Occam's razor*\n","  * Given two explanations for something, the explanation most likely to be correct is the simplest one - the one that makes fewer assumptions.\n","  \n","* A *simple model* in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters).\n","\n","* A common way to mitigate overfitting is to put contraints on the complexity of a network by forcing its weights to take only small values, which makes the distribution of weight values more *regular*.\n","\n","* This is called *weight regularization*.\n","  * It is done by adding to the loss function of the network a *cost* associated with having large weights.\n","  * *L1 regularization*\n","    * The cost added is proportional to the absolute value of the weight coefficients.\n","    * The *L1 norm* of the weights\n","  * *L2 regularization*\n","    * The cost added is proportional to the square of the value of the weight coefficients.\n","    * The *L2 norm* of the weights\n","    * It is also called *weight decay* in the context of neural networks.\n","    \n","* L2 weight regularization in Keras\n","\n","  ```python\n","    from tensorflow.keras import regularizers\n","\n","    model = models.Sequential() \n","    model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,))) \n","    #0.001은 l1regularazation에 대한 합에 앞에 붙는 계수를 의미한다\n","    model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu')) \n","    model.add(layers.Dense(1, activation='sigmoid'))\n","  ```\n","\n","  * The impact of the L2 regularization\n","  \n","    <img src=\"https://drive.google.com/uc?id=15hDvTnLTCtd-jl6p4k4VQwYLWLEw-E8Y\" width=\"500\">\n","  \n","  "]},{"cell_type":"markdown","metadata":{"id":"tKkP7Ak7_1y4"},"source":["> ### Adding dropout"]},{"cell_type":"markdown","source":["뉴럴네트워크에 특화된 regulrazation이다<br>당연히 과적합을 막기위해서 사용<br>\n","학습과정중에 hidden노드를 껐다 켰다 그렇게 한다<br>\n","그렇게 각각의 노드가 꺼질 확률을 hyperparamet로 조정하는 느낌이다<br>\n","중요한건 매번 iteration마다 랜덤한 확률로 살아남는것이다<br>\n","또 중요한건 test에서는 dropout이 사라진다 그걸 켜놓으면 모델이 fit할때마다 다른 결과가 나오기 떄문이다 <br>\n","dropout을 걷어내고 그냥 아무것도 안하면 dropout을 한 의미가 없다 그래서 확률로써 나올수 있는 값을계산하고 test값에 그만큼의 계수를 곱해준다"],"metadata":{"id":"7UbvECTjTP0z"}},{"cell_type":"markdown","metadata":{"id":"E5n37wYg_4df"},"source":["* *Dropout* is one of the most effective and most commonly used regularization techniques for neural networks.\n","\n","* It consists of randomly dropping out (setting to zero) a number of output features of the layer during training.\n","  * E.g., [0.2, 0.5, 1.3, 0.8, 1.1] --> (dropout) --> [0, 0.5, 1.3, 0, 1.1]\n","  \n","* The *dropout rate* is the fraction of the features that are zeroed out.\n","\n","* At test time, no units are dropped out.\n","  * Instead, the layer's output values are scaled down by a factor equal to *(1-the dropout rate)* to balance for the fact that more units are active than at training time.\n","  ><img src=\"https://drive.google.com/uc?id=1nfP0HxqbBcW-isMbCDuDkD-Bu_x7w65_\" width=\"800\">\n","  \n","* Implementation using Numpy\n","\n","  ```python\n","    # At training time, we zero out 50% of activations.\n","    layer_output *= np.random.randint(0, high=2, size=layer_output.shape)\n","\n","    # At test time, we scale down the output.\n","    layer_output *= 0.5\n","  ```\n","\n","* Another implementation (in practice)\n","\n","  ```python\n","    # At training time\n","    layer_output *= np.random.randint(0, high=2, size=layer_output.shape) \n","    layer_output /= 0.5\n","  ```\n","\n","* In Tensorflow,\n","\n","  ```python\n","    model.add(layers.Dropout(0.5))\n","  ```\n","\n","* Adding dropout to the IMDB network\n","\n","  ```python\n","    model = models.Sequential() \n","    model.add(layers.Dense(16, activation='relu', input_shape=(10000,))) \n","    model.add(layers.Dropout(0.5)) \n","    model.add(layers.Dense(16, activation='relu')) \n","    model.add(layers.Dropout(0.5)) \n","    model.add(layers.Dense(1, activation='sigmoid'))\n","  ```\n","\n","><img src=\"https://drive.google.com/uc?id=15i6-m5KQvETAyj2BOjMoAZvO9jFqKYuT\" width=\"500\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Qeq7jVMzJxCR"},"source":["## The universal workflow of machine learning"]},{"cell_type":"markdown","metadata":{"id":"LKNCw6r_J2bF"},"source":["> ### Defining the problem and assembling a dataset"]},{"cell_type":"markdown","metadata":{"id":"uguleNqnKC1T"},"source":["* First, we must define the problem at hand:\n","  * What will your input data be?\n","  * What are you trying to predict?\n","  * What type of problem are you facing? \n","\n","* The hypotheses you make at this stage:\n","  * The outputs can be predicted given the inputs.\n","  * The available data is sufficiently informative to learn the relationship between inputs and outputs.\n","  \n","* Not all problems can be solved: a dataset (X, Y) doesn't mean X contains enough information to predict Y.\n","\n","* Keep in mind that machine learning can only be used to learn patterns that are present in the training data.\n","  * For instance, using machine learning trained on past data to predict the future is making the assumption that the future will behave like the past."]},{"cell_type":"markdown","metadata":{"id":"25q8UlRXLfR9"},"source":["> ### Deciding on an evaluation protocol"]},{"cell_type":"markdown","metadata":{"id":"vMIQf6rlLiqK"},"source":["* How you will measure the current progress\n","  * Maintaining a hold-out validation set\n","  * Doing k-fold cross validation\n","  * Doing iterated k-fold validation\n"," "]},{"cell_type":"markdown","metadata":{"id":"HL8fSErsL038"},"source":["> ### Preparing the data"]},{"cell_type":"markdown","metadata":{"id":"3uLmpkHOL39L"},"source":["* Once you know what you’re training on, what you’re optimizing for, and how to evaluate your approach, you’re almost ready to begin training models.\n","\n","* Formatting the data\n","  * The data should be formatted as tensors.\n","  * The values taken by these tensors should be scaled to small values.\n","  * If different features take values in different ranges, then the data should be normalized.\n","  * Some feature engineering may be needed, especially for small-data problems."]},{"cell_type":"markdown","source":["0근처로 normalized를 해줘야한다"],"metadata":{"id":"oCUnUSCcaA3K"}},{"cell_type":"markdown","metadata":{"id":"9pOjeoz5MphX"},"source":["> ### Developing a model that does better than a baseline"]},{"cell_type":"markdown","metadata":{"id":"TSdNuFRLMuc8"},"source":["* Developing a small model that is capable of beating a dumb baseline\n","\n","* Three key choices to build the network:\n","  * Last-layer activation\n","  * Loss function\n","  * Optimization configuration\n","  \n","><img src=\"https://drive.google.com/uc?id=15lpFAc2T95-g4GiugmRdYnuUVK_24c9H\" width=\"700\">"]},{"cell_type":"markdown","source":["뭘할지에 따라 loss function이나 activation function 무엇을 쓸지 거의 정해져 있따"],"metadata":{"id":"5FlASijQaiWB"}},{"cell_type":"markdown","metadata":{"id":"AFFja-iSNYqG"},"source":["> ### Scaling up: developing a model that  overfits"]},{"cell_type":"markdown","source":["underfit이 훨씬 critical하다 차라리 overfit을 시켜서 줄이는과정이 훨씬 좋다"],"metadata":{"id":"J0w35fWjbbuq"}},{"cell_type":"markdown","metadata":{"id":"1u2u_IvNNcLD"},"source":["* Once you’ve obtained a model that has statistical power, the question becomes, is your model sufficiently powerful?\n","\n","* Developing a model that overfits:\n","  * Add more layers\n","  * Make the layers bigger\n","  * Train for more epochs\n","  \n","* Always monitor the training loss and validation loss, as well as the training and validation values for any metrics you care about."]},{"cell_type":"markdown","metadata":{"id":"NT8AlIlqN66R"},"source":["> ### Reguralizing the model and tuning the hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"KKJ-Ng9rN_T_"},"source":["* Repeatedly modify the model, train it, evaluate on the validation data, again and again.\n","\n","* We can try:\n","  * Add dropout\n","  * Try different architectures\n","  * Add regularization terms\n","  * Try different hyperparameters\n","  * Optionally, iterate on feature engineering\n","  \n","* Keep in mind that every time you use feedback from your validation process to tune your model, you leak information about the validation process into the model.\n","\n","* Once you’ve developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set."]}]}