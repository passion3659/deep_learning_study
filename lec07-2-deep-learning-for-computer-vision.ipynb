{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lec07-2-deep-learning-for-computer-vision.ipynb","provenance":[{"file_id":"1TaAnOrSJr0rPN_5VBPj78fUr6yuZglUl","timestamp":1554096971868},{"file_id":"1-GA4RXQPyAGWkvdBMBBGzOj61nUDNQes","timestamp":1552904168896}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dbGDwo4AQFi-"},"source":["## 5.1 Introduction to convnets (CNNs)"]},{"cell_type":"markdown","metadata":{"id":"agQMvaVead7Q"},"source":["* A simple convnet example"]},{"cell_type":"code","metadata":{"id":"veYd3GskUJlW"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras import models\n","\n","model = models.Sequential() \n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))  #parameter (3*3+1)*32\n","model.add(layers.MaxPooling2D((2, 2))) #(2,2)에서 첫번째 2는 \n","model.add(layers.Conv2D(64, (3, 3), activation='relu')) #parameter는 (3*3*32+1)*64 여기서 잘 봐야하는데 kernel을 3*3을 줬다고 그게 다가 아닌 input shape의 32도 생각해서 kernel하나는 (3*3*32+1)이다\n","model.add(layers.MaxPooling2D((2, 2))) \n","model.add(layers.Conv2D(64, (3, 3), activation='relu')) #(3*3*64+1)*64"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lpeli0OBUTSl"},"source":["* CNN takes as input tensors of shape `(image_height, image_width, image_channels)` (not including the batch dimension)."]},{"cell_type":"code","metadata":{"id":"fJnkd_jKUqez","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650886263518,"user_tz":-540,"elapsed":923,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"3dc348f0-4b37-4bac-f76d-aa69ea514a0e"},"source":["model.summary() #output shape의 첫 인자가 다 none으로 나와있는데 첫번째 표현은 batch에 관한 것이다"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 26, 26, 32)        320       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n","                                                                 \n","=================================================================\n","Total params: 55,744\n","Trainable params: 55,744\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"PdXsowuoUser"},"source":["* Note that the output of every `Conv2D` and `MaxPooling2D` layer is a 3D tensor of shape `(height, width, channels)`.\n","* The width and height dimensions tend to shrink as you go deeper in the network.\n","* The number of channels is controlled by the first argument passed to the `Conv2D` layers (32 or 64)."]},{"cell_type":"code","metadata":{"id":"jiP6xihuVQKf"},"source":["model.add(layers.Flatten()) #일종의 operation인데 3d tensor를 id tensor로 쭉 나열하겠다는 의미이다\n","model.add(layers.Dense(64, activation='relu'))\n","model.add(layers.Dense(10, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UB1VFLioVYe9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650887882517,"user_tz":-540,"elapsed":10,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"7f204582-0d17-49c6-cfd9-ae3c180cb8b1"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 26, 26, 32)        320       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n","                                                                 \n"," flatten (Flatten)           (None, 576)               0         \n","                                                                 \n"," dense (Dense)               (None, 64)                36928     \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 93,322\n","Trainable params: 93,322\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"irWn5TuXVZcq"},"source":["* Training the CNN on the MNIST digits"]},{"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bX0BOi5rmcXC","executionInfo":{"status":"ok","timestamp":1650888141922,"user_tz":-540,"elapsed":706,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"a18b8b67-6c10-4888-8c36-c15080e512bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["train_images.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RbDeuq1dmgEp","executionInfo":{"status":"ok","timestamp":1650888175486,"user_tz":-540,"elapsed":267,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"2314d01f-c05b-4b6c-8a4b-1900eea4aeb0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 28, 28)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"C4orkwmnVpgB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650888415879,"user_tz":-540,"elapsed":42663,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"02764ca3-e266-4a6d-f686-e2ae8b981baa"},"source":["from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","train_images = train_images.reshape((60000, 28, 28, 1)) #원래는 (60000,28,28)을 (60000,28,28,1)로 바꿔주는 작업이다\n","train_images = train_images.astype('float32') / 255 #그리고 0부터 255의 정수로 되어있는 데이터 값들을 실수로 변경후 0부터1까지의 값으로 변경시킨다.\n","\n","test_images = test_images.reshape((10000, 28, 28, 1)) \n","test_images = test_images.astype('float32') / 255\n","\n","train_labels = to_categorical(train_labels) #one-hot encoding실행\n","test_labels = to_categorical(test_labels)\n","\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) #metrics는 monitoring하는 것을 본다는건데 지금은 accuracy를 보겠다는 의미이다. \n","model.fit(train_images, train_labels, epochs=5, batch_size=64) #64라는건 한번 업데이트할때 랜덤하게 뽑은 64개의 데이터를 기준으로 gradient 계산후 업데이트한다는 의미이다."],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","938/938 [==============================] - 17s 5ms/step - loss: 0.1714 - accuracy: 0.9460\n","Epoch 2/5\n","938/938 [==============================] - 5s 5ms/step - loss: 0.0456 - accuracy: 0.9856\n","Epoch 3/5\n","938/938 [==============================] - 5s 5ms/step - loss: 0.0320 - accuracy: 0.9897\n","Epoch 4/5\n","938/938 [==============================] - 5s 5ms/step - loss: 0.0244 - accuracy: 0.9927\n","Epoch 5/5\n","938/938 [==============================] - 5s 5ms/step - loss: 0.0193 - accuracy: 0.9938\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fd3d40a5490>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"WRM5FFuaXGUu"},"source":["test_loss, test_acc = model.evaluate(test_images, test_labels)\n","test_acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ES-0w15WqfsX"},"source":["> ### The convolution operation"]},{"cell_type":"markdown","metadata":{"id":"q4WYAQ1faDPG"},"source":["* The difference between a densely connected layer and a convolution layer\n","  * `Dense` layers learn global patterns in their input feature space.\n","  * Convolution layers learn local patterns: in the case of images, patterns found in small 2D windows of the inputs.\n","  \n","   ><img src=\"https://drive.google.com/uc?id=15vIiBlYgRb94J-nS7z9_n47W5m5kO-n2\" width=\"300\">\n","  \n","* Properties of CNNs\n","  * The patterns they learn are translation invariant.<br>\n","  translation에 invariant하다는것이 아주 중요한 특징이다<br>\n","  앞서서 convolutional layer에 equivalient하다는 것은 translation하고 convolution한 결과가 convolution하고 translation한 결과와 같다는 것이다.\n","  <br>translation invariant하다는건 translation을 하고도 결과가 동일하다는 것이다<br> invariant와 equivalient은 같은말이 아니다<br>convolution은 translation에 equivalient한데 convolution을 여러개 쌓은 convolutional network는 convolution에 invariant합니다.<br>뭔소리야... 8주차 첫번째 강의 20분쯤 다시 보자\n","  * They can learn spatial hierarchies of patterns.\n","  \n","  ><img src=\"https://drive.google.com/uc?id=163vTpjJeDI7rPPwIcjAFZlK1l7vb83WV\" width=\"400\">\n","  \n","* Convolutions operate over 3D tensors, called *feature maps*, with two spatial axes (*height* and *width*) as well as *depth* axis (also called the *channels* axis).\n","  * For an RGB image, the dimension of the depth axis is 3.\n","  \n","* The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an *output feature map*.\n","  * The output feature map is still a 3D tensor with user-specified depth.\n","  * The different channels in that depth axis stand for *filters*.<br>\n","  convolution하는 필터의 개수로 depth가 정해진다이\n","  \n","  ><img src=\"https://drive.google.com/uc?id=164ODzRvJ43VcwgIRKT6T88sFG4T6Pqs_\" width=\"600\">\n","  \n","* Convolutions are defined by two key parameters.(두개의 핵심 hyper parameter)\n","  * *Size of the patches extracted from the inputs*: Typically, 3\\*3 or 5\\*5(kernel 크기)\n","  * *Depth of the output feature map* (필터개수 다음레이어의 depth결정)\n","  \n","* A convolution works by sliding the windows of size 3\\*3 or 5\\*5 over the 3D input feature map.\n","  * At every possible location, it extracts the 3D patch of surrounding features, then transforms (via a tensor product with the same learned weight matrix, called the *convolution kernel*) 3D patch into a 1D vector of shape `(output_depth,)`.\n","  * All of these vectors are then spatially reassembled into a 3D output map of shape `(height, width, output_depth)`.\n","  * Every spatial location in the output feature map corresponds to the same location in the input feature map.\n","\n","  ><img src=\"https://drive.google.com/uc?id=16I-SdIdrbCbIVIDoSs1pbAoO6Otry9nY\" width=\"600\"><br>위의 dot product의 결과는 kernel에 의해서이루어진건데 (3*3)kernel의 필터가 3개 있어서 저런 결과가 나온거겠쬬?<br>input (5,5,2)에 (3,(3,3))인 hyperparameter를 수행해서 (3,3,3)이 나온거다.<br> (3,3,3)에서 앞의 두개의 3은 5-3+1의 연산이 나온거고 제일 뒤의 3은 filter개수의 3에서 나온것이다\n","  \n","* Note that the output width and height may differ from the input width and height.\n","  * Border effects<br>\n","  여기서의 border effect는 그 stride했을때 버리는 부분 말하는거<br>\n","  예를 들어 3*3 input에 2*2kernel로 stride를 2를 하면 뒤에 한줄 버리는거 의미\n","  * The use of *strides*\n","  "]},{"cell_type":"markdown","metadata":{"id":"FvzO66QFahFw"},"source":["* **Border effects and padding**<br>\n","그냥 아래는 5\\*5의 input을 3\\*3으로 convolution을 하면 3\\*3이 나오는데 output을 5\\*5로 만들려면 zero padding을 해서 맞춘다는것이다\n","\n","  * Consider a 5\\*5 feature map and convolution operation with kernel size 3\\*3. Then, the output feature map will be 3\\*3.\n","\n","  ><img src=\"https://drive.google.com/uc?id=168uo4gVzAYTHs1THFG-mMWYyM4jz5JOw\" width=\"700\">\n","  \n","  * If you want to get an output feature map with the same spatial dimensions as the input, you can use *padding*.\n","    * Padding consists of adding an appropriate number of rows and columns on each side of the input feature map.\n","    \n","  ><img src=\"https://drive.google.com/uc?id=16AP8rF498xwxNkjAkTA78Ny7mAOav0xF\" width=\"700\">\n","    \n","  * In `Conv2D` layers, padding is configurable via the `padding` argument, which takes two values: `valid` and `same`.\n","  <br>이건 뭔소리냐면 conv2d에 구현되어있는 모델에 padding의 값을 valid와 same으로 설정할수 있는데<br> valid는 padding없이 유효한 영역만 한다는 의미이고<br> same은 input과 output의 크기가 같다는 것을 의미한다.\n","    \n","    "]},{"cell_type":"markdown","metadata":{"id":"g9MUB0gKb0B7"},"source":["* **Convolution strides**\n","\n","  * The distance between two successive windows is a parameter of the convolution, called its *stride*.\n","  \n","  * It is possible to have *strided convolutions*: convolutions with a stride higher than 1.\n","\n","  ><img src=\"https://drive.google.com/uc?id=16M-MZjLIS0qpZpzVILlwrYuA8X5xcEq8\" width=\"700\">\n","  \n","  * Using stride 2 means that the width and height of the feature map are downsampled by a factor of 2.\n","  \n","  * To downsample feature maps, we can also use the *max-pooling* operations.\n"," "]},{"cell_type":"markdown","metadata":{"id":"cVk9ih1deqCg"},"source":["> ### The max-pooling operation"]},{"cell_type":"markdown","metadata":{"id":"_I1VIiW0eyc_"},"source":["* Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel.\n","  * It is conceptually similar to convolution, except that instead of transforming local patches via a learned linear transformation (the convolution kernel), they are transformed via a hardcoded `max` tensor operation.\n","  \n","* Max pooling is usually done with 2\\*2 windows and stride 2, in order to downsample the feature maps by a factor of 2.\n","\n","* Why downsample feature maps?"]},{"cell_type":"code","metadata":{"id":"noWlfImndyl9"},"source":["model_no_max_pool = models.Sequential() \n","model_no_max_pool.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) \n","model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu')) \n","model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation='relu'))\n","\n","model_no_max_pool.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rzfWfsr_d5eD"},"source":["* The reason to use downsampling is to reduce the number of feature-map coefficients to process, as well as induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows.\n","\n","* Max pooling is not the only way to downsampling.\n","  * Strided convolutions, average pooling, etc."]},{"cell_type":"markdown","metadata":{"id":"5xaClvoCfU5u"},"source":["## 5.2 Training a convnet from scratch on a small dataset"]},{"cell_type":"markdown","metadata":{"id":"EZP0gEIigT19"},"source":["* Having to train an image-classification model using very little data is a common situation.\n","\n","* Here, we will review several strategies to tackle the small dataset problem.\n","  * Data augmentation\n","  * Feature extraction with a pretrained network\n","  * Fine-tuning a pretrained network\n","  \n","* The Dogs vs. Cats dataset (https://www.kaggle.com/c/dogs-vs-cats/data)\n","  * Download URL: https://drive.google.com/uc?id=1AmgANN-SJmCMtLs6CTsZOyY9_W5DVCMT\n","  * Medium-resolution color JPEGs\n","  * 25,000 images of dogs and cats (12,500 from each class)\n","  * We will use a subset of this dataset.\n","    * A training set with 1,000 samples of each class\n","    * A validation set with 500 samples of each class\n","    * A test set with 500 samples of each class\n","    \n","* **Load the dataset**"]},{"cell_type":"code","metadata":{"id":"hm02HTcCufEv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944496124,"user_tz":-540,"elapsed":20209,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"5c11f2e3-f99c-41ff-82fb-083df0c24362"},"source":["# mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# unzip\n","import zipfile, os, shutil\n","\n","dataset = '/content/gdrive/My Drive/deep_learning/deep_learning_lecture/dogs_vs_cats_subset.zip'\n","dst_path = '/content/dogs_vs_cats_subset'  ##이부분이 가장 중요한데 데이터의 수가 너무 많으면 구글 드라이브에 안들어간다. 그래서 구글 인스턴스인 content에 저장해서 불러오는 방식을 사용한다\n","dst_file = os.path.join(dst_path, 'dogs_vs_cats_subset.zip')\n","\n","if not os.path.exists(dst_path):\n","  os.makedirs(dst_path)\n","\n","# copy zip file\n","shutil.copy(dataset, dst_file)\n","  \n","with zipfile.ZipFile(dst_file, 'r') as file:\n","  file.extractall(dst_path)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["%cd /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lRG4NkGlRlqR","executionInfo":{"status":"ok","timestamp":1650944498920,"user_tz":-540,"elapsed":317,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"fb662b00-7936-4a79-895d-de0c169fa920"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","metadata":{"id":"pmPAJVpdN1Ft","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944500136,"user_tz":-540,"elapsed":6,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"5674d7e7-e4b2-456c-98a9-63d41ce8c648"},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","metadata":{"id":"R44JriuTN2_M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944501827,"user_tz":-540,"elapsed":381,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"af0d7cd5-ddf5-4647-d0df-049cba579674"},"source":["!ls -al"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 24\n","drwxr-xr-x 1 root root 4096 Apr 26 03:41 .\n","drwxr-xr-x 1 root root 4096 Apr 26 03:36 ..\n","drwxr-xr-x 1 root root 4096 Apr 19 14:22 .config\n","drwxr-xr-x 4 root root 4096 Apr 26 03:41 dogs_vs_cats_subset\n","drwx------ 5 root root 4096 Apr 26 03:41 gdrive\n","drwxr-xr-x 1 root root 4096 Apr 19 14:23 sample_data\n"]}]},{"cell_type":"code","metadata":{"id":"NwDqXuelN7KR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944502800,"user_tz":-540,"elapsed":3,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"2ea18df5-c04d-420c-d5be-55dc1410c9a6"},"source":["%cd dogs_vs_cats_subset/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/dogs_vs_cats_subset\n"]}]},{"cell_type":"code","metadata":{"id":"W1zpctN0OOJr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944504235,"user_tz":-540,"elapsed":434,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"93091428-12f5-41a3-89a3-ed918ab05bcb"},"source":["!ls -al"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 88752\n","drwxr-xr-x 4 root root     4096 Apr 26 03:41 .\n","drwxr-xr-x 1 root root     4096 Apr 26 03:41 ..\n","-rw------- 1 root root 90863632 Apr 26 03:41 dogs_vs_cats_subset.zip\n","drwxr-xr-x 3 root root     4096 Apr 26 03:41 __MACOSX\n","drwxr-xr-x 5 root root     4096 Apr 26 03:41 subset\n"]}]},{"cell_type":"code","metadata":{"id":"VIWv0Ru-OQz-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944505917,"user_tz":-540,"elapsed":282,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"ad22a312-1f14-49db-92c6-8b422ae94b87"},"source":["%cd subset"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/dogs_vs_cats_subset/subset\n"]}]},{"cell_type":"code","metadata":{"id":"tnsGH600OTDW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944507078,"user_tz":-540,"elapsed":5,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"8582d79b-9a27-48e1-a89f-61f6d8bca3e5"},"source":["!ls -al"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 36\n","drwxr-xr-x 5 root root  4096 Apr 26 03:41 .\n","drwxr-xr-x 4 root root  4096 Apr 26 03:41 ..\n","-rw-r--r-- 1 root root 12292 Apr 26 03:41 .DS_Store\n","drwxr-xr-x 4 root root  4096 Apr 26 03:41 test\n","drwxr-xr-x 4 root root  4096 Apr 26 03:41 train\n","drwxr-xr-x 4 root root  4096 Apr 26 03:41 validation\n"]}]},{"cell_type":"code","metadata":{"id":"xFXxS9KUOUWt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944508542,"user_tz":-540,"elapsed":4,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"ef0d1f5b-9ed6-460b-9ccb-9423e6792c0a"},"source":["%cd train"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/dogs_vs_cats_subset/subset/train\n"]}]},{"cell_type":"code","metadata":{"id":"LMq8R0jrOYQg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944509758,"user_tz":-540,"elapsed":5,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"b1dc2fd7-0f2c-4195-962c-1ef568dd4e56"},"source":["!ls -al"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 92\n","drwxr-xr-x 4 root root  4096 Apr 26 03:41 .\n","drwxr-xr-x 5 root root  4096 Apr 26 03:41 ..\n","drwxr-xr-x 2 root root 36864 Apr 26 03:41 cats\n","drwxr-xr-x 2 root root 36864 Apr 26 03:41 dogs\n","-rw-r--r-- 1 root root  8196 Apr 26 03:41 .DS_Store\n"]}]},{"cell_type":"code","metadata":{"id":"7Yf_Pi5iuio4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944511456,"user_tz":-540,"elapsed":426,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"a4b18e36-7f8e-4275-9945-52a531c132c1"},"source":["train_cats_dir = os.path.join(dst_path, 'subset/train/cats')\n","train_dogs_dir = os.path.join(dst_path, 'subset/train/dogs')\n","\n","validation_cats_dir = os.path.join(dst_path, 'subset/validation/cats')\n","validation_dogs_dir = os.path.join(dst_path, 'subset/validation/dogs')\n","\n","test_cats_dir = os.path.join(dst_path, 'subset/test/cats')\n","test_dogs_dir = os.path.join(dst_path, 'subset/test/dogs')\n","\n","print('total training cat images:', len(os.listdir(train_cats_dir)))\n","print('total training dog images:', len(os.listdir(train_dogs_dir)))\n","\n","print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n","print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n","\n","print('total test cat images:', len(os.listdir(test_cats_dir)))\n","print('total test dog images:', len(os.listdir(test_dogs_dir)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total training cat images: 1000\n","total training dog images: 1000\n","total validation cat images: 500\n","total validation dog images: 500\n","total test cat images: 500\n","total test dog images: 500\n"]}]},{"cell_type":"markdown","metadata":{"id":"vOUT7sPEvQBq"},"source":["* **Building the network**\n","  * Note that we are dealing with bigger images and a more complex problem than MNIST.\n","  * We will make the network larger.\n","  * Here, we start from inputs of size 150\\*150, and end up with feature maps of size 7\\*7 just before the `Flatten` layer.\n","  * The depth of the feature maps progressively increases in the network, whereas the size of the feature maps decreases. This is a pattern you'll see in almost all CNNs.\n"]},{"cell_type":"code","metadata":{"id":"iq1fKHnBw_m3"},"source":["from tensorflow.keras import layers \n","from tensorflow.keras import models\n","\n","model = models.Sequential() \n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3))) \n","model.add(layers.MaxPooling2D((2, 2))) \n","model.add(layers.Conv2D(64, (3, 3), activation='relu')) \n","model.add(layers.MaxPooling2D((2, 2))) \n","model.add(layers.Conv2D(128, (3, 3), activation='relu')) \n","model.add(layers.MaxPooling2D((2, 2))) \n","model.add(layers.Conv2D(128, (3, 3), activation='relu')) \n","model.add(layers.MaxPooling2D((2, 2))) \n","model.add(layers.Flatten()) \n","model.add(layers.Dense(512, activation='relu')) \n","model.add(layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cOE2CfftxIJO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944522586,"user_tz":-540,"elapsed":367,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"b7fb028c-9434-4cdc-b76e-3de48f137b42"},"source":["model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 148, 148, 32)      896       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 36, 36, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 17, 17, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 15, 15, 128)       147584    \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n"," 2D)                                                             \n","                                                                 \n"," flatten (Flatten)           (None, 6272)              0         \n","                                                                 \n"," dense (Dense)               (None, 512)               3211776   \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 513       \n","                                                                 \n","=================================================================\n","Total params: 3,453,121\n","Trainable params: 3,453,121\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"IXgbSNKrxNha"},"source":["from tensorflow.keras import optimizers\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.RMSprop(learning_rate=1e-4),\n","              metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0FIMme1mxdmo"},"source":["* **Data preprocessing**\n","\n","  * Currently, the data is stored on a drive as JPEG files. So we need the following steps:\n","    * Read the picture files.\n","    * Decode the JPEG content to RGB grids of pixels.\n","    * Convert these into floating-point tensors.\n","    * Rescale the pixel values (between 0 and 255) to the `[0,1]` inverval.\n","    \n","  * Keras has a module with image-processing helper tools, located as `keras.preprocesseing.image`.\n","  \n","  * In particular, it contains the class `ImageDataGenerator`, which lets us quickly set up Python generators that can automatically turn image files on disk into batches of preprocessed tensors."]},{"cell_type":"code","metadata":{"id":"th4x_cVayWWE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944527754,"user_tz":-540,"elapsed":422,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"d19c5e7f-c17e-4d76-8769-3173df7617a0"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","train_dir = os.path.join(dst_path, 'subset/train')\n","validation_dir = os.path.join(dst_path, 'subset/validation')\n","\n","train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(train_dir,\n","                                                    target_size=(150,150), #100*100이면 키150*150으로 키워주고 200*200이면 150*150으로 줄여준다\n","                                                    batch_size=20,      #넌 뭐임??\n","                                                    class_mode='binary') # class mode는 class가 무엇인지 판단하고 label을 어떻게 할지에 대해서이다.\n","                                                                          #여기서는 개와 고양이 두개의 class라서 0또는 1로 label를 하는것이다.\n","\n","validation_generator = test_datagen.flow_from_directory(validation_dir, \n","                                                        target_size=(150,150),\n","                                                        batch_size=20,\n","                                                        class_mode='binary')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n"]}]},{"cell_type":"code","metadata":{"id":"dvuN8L6yzBIR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944534496,"user_tz":-540,"elapsed":278,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"3d4feefc-9581-48cb-c091-d0bb6005c508"},"source":["for data_batch, labels_batch in train_generator:\n","  print('data batch shape:', data_batch.shape)\n","  print('labels batch shape:', labels_batch.shape)\n","  break"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data batch shape: (20, 150, 150, 3)\n","labels batch shape: (20,)\n"]}]},{"cell_type":"code","metadata":{"id":"W5hd5cZvN1hR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944536839,"user_tz":-540,"elapsed":269,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"c7f225e7-f596-4fc2-c9c9-6e468a16a735"},"source":["labels_batch"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n","       0., 1., 0.], dtype=float32)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"5N-TOfWrN4z4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650944542173,"user_tz":-540,"elapsed":288,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"ac94c58e-b513-4e0b-ac2a-0bd6d2762270"},"source":["print(data_batch[0].shape, data_batch[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(150, 150, 3) [[[0.7411765  0.5921569  0.43921572]\n","  [0.7372549  0.5882353  0.43529415]\n","  [0.7294118  0.5803922  0.427451  ]\n","  ...\n","  [0.5568628  0.5568628  0.40784317]\n","  [0.5568628  0.5568628  0.40784317]\n","  [0.5568628  0.5568628  0.40784317]]\n","\n"," [[0.7176471  0.5686275  0.4156863 ]\n","  [0.7137255  0.5647059  0.41176474]\n","  [0.7058824  0.5568628  0.4039216 ]\n","  ...\n","  [0.5568628  0.5568628  0.40784317]\n","  [0.5568628  0.5568628  0.40784317]\n","  [0.5568628  0.5568628  0.40784317]]\n","\n"," [[0.70980394 0.5568628  0.4039216 ]\n","  [0.7019608  0.54901963 0.39607847]\n","  [0.69803923 0.54509807 0.3921569 ]\n","  ...\n","  [0.5568628  0.5568628  0.40784317]\n","  [0.5568628  0.5568628  0.40784317]\n","  [0.5568628  0.5568628  0.40784317]]\n","\n"," ...\n","\n"," [[0.654902   0.5294118  0.32941177]\n","  [0.65882355 0.53333336 0.34117648]\n","  [0.6666667  0.5411765  0.34901962]\n","  ...\n","  [0.7294118  0.6156863  0.45098042]\n","  [0.7294118  0.6156863  0.45098042]\n","  [0.7294118  0.6156863  0.45098042]]\n","\n"," [[0.654902   0.5294118  0.32941177]\n","  [0.65882355 0.53333336 0.34117648]\n","  [0.6666667  0.5411765  0.34901962]\n","  ...\n","  [0.7294118  0.6156863  0.45098042]\n","  [0.7294118  0.6156863  0.45098042]\n","  [0.7294118  0.6156863  0.45098042]]\n","\n"," [[0.654902   0.5294118  0.32941177]\n","  [0.65882355 0.53333336 0.34117648]\n","  [0.6666667  0.5411765  0.34901962]\n","  ...\n","  [0.7294118  0.6156863  0.45098042]\n","  [0.7294118  0.6156863  0.45098042]\n","  [0.7294118  0.6156863  0.45098042]]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"AUnWNNH6zWUg"},"source":["* Let's fit the model to the data using the generator: `fit_generator` method.\n","  * It expects as its first argument a Python generator that will yield batches of inputs and targets indefinitely.\n","  * Because the data is being generated endlessly, the Keras model needs to know how many samples to draw from the generator before declaring an epoch over.\n","    * This is the role of the `steps_per_epoch` argument.\n","    * In this case, batches are 20 samples, so it will take 100 batches for an epoch.\n","    * Similarly, `validation_steps` argument is required if you pass a generator as `validation_data`.<br> 여기부분은 다시 들어보자 2차시 28분 먼소리인지 모르겠다"]},{"cell_type":"code","metadata":{"id":"hJLyQ-3a0ix-","colab":{"base_uri":"https://localhost:8080/","height":474},"executionInfo":{"status":"error","timestamp":1650945073114,"user_tz":-540,"elapsed":28573,"user":{"displayName":"양정열","userId":"01296504213550964769"}},"outputId":"a5ce7fd0-00f4-459c-db88-9efc2d64395c"},"source":["history = model.fit_generator(train_generator,\n","                              steps_per_epoch=100, # 한 epoch 당 몇번의 iteration을 돌아야하는가\n","                              epochs=30,\n","                              validation_data=validation_generator,\n","                              validation_steps=50)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  \"\"\"\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","100/100 [==============================] - 23s 120ms/step - loss: 0.6879 - acc: 0.5410 - val_loss: 0.6832 - val_acc: 0.5010\n","Epoch 2/30\n"," 48/100 [=============>................] - ETA: 4s - loss: 0.6664 - acc: 0.6031"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-df6d61bb9972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                               validation_steps=50)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2221\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2222\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2223\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_generate_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"B1UBE_A70w7z"},"source":["model.save('cats_and_dogs_subset_1.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HaLNM4DG8yIO"},"source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['acc'] \n","val_acc = history.history['val_acc'] \n","loss = history.history['loss'] \n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc') \n","plt.plot(epochs, val_acc, 'b', label='Validation acc') \n","plt.title('Training and validation accuracy') \n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss') \n","plt.plot(epochs, val_loss, 'b', label='Validation loss') \n","plt.title('Training and validation loss') \n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kVme8m1L89X8"},"source":["* **Data  augmentation**\n","\n","  * Overfitting is caused by having too few samples to learn from, rendering you unable to train a model that can generalize to new data.\n","\n","  * Data augmentation takes the approach of generating more training data from existing training samples, by *augmenting* the samples via a number of random transformations."]},{"cell_type":"code","metadata":{"id":"QKjXgQL39fGV"},"source":["datagen = ImageDataGenerator(rotation_range=40,\n","                             width_shift_range=0.2,\n","                             height_shift_range=0.2,\n","                             shear_range=0.2,\n","                             zoom_range=0.2,\n","                             horizontal_flip=True,\n","                             fill_mode='nearest')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVvSu56r-K-s"},"source":["import matplotlib.pyplot as plt\n","from tensorflow.keras.preprocessing import image\n","\n","fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n","\n","img_path = fnames[3]\n","\n","img = image.load_img(img_path, target_size=(150,150))\n","\n","x = image.img_to_array(img)\n","x = x.reshape((1,) + x.shape)\n","\n","i=0\n","for batch in datagen.flow(x, batch_size=1):\n","  plt.figure(i)\n","  imgplot = plt.imshow(image.array_to_img(batch[0]))\n","  i += 1\n","  if i%4 == 0: break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zw_ZVT2b-meJ"},"source":["model = models.Sequential() \n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3))) \n","model.add(layers.MaxPooling2D((2, 2))) \n","model.add(layers.Conv2D(64, (3, 3), activation='relu')) \n","model.add(layers.MaxPooling2D((2, 2))) \n","model.add(layers.Conv2D(128, (3, 3), activation='relu')) \n","model.add(layers.MaxPooling2D((2, 2))) \n","model.add(layers.Conv2D(128, (3, 3), activation='relu')) \n","model.add(layers.MaxPooling2D((2, 2))) \n","model.add(layers.Flatten()) \n","model.add(layers.Dropout(0.5)) # add dropout\n","model.add(layers.Dense(512, activation='relu')) \n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=1e-4), \n","              metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kz9UM76A_XzZ"},"source":["train_datagen = ImageDataGenerator(rescale=1./255,\n","                                   rotation_range=40,\n","                                   width_shift_range=0.2,\n","                                   height_shift_range=0.2,\n","                                   shear_range=0.2,\n","                                   zoom_range=0.2,\n","                                   horizontal_flip=True,\n","                                   fill_mode='nearest')\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(train_dir,\n","                                                    target_size=(150,150),\n","                                                    batch_size=20,\n","                                                    class_mode='binary')\n","\n","validation_generator = test_datagen.flow_from_directory(validation_dir,\n","                                                        target_size=(150,150),\n","                                                        batch_size=20,\n","                                                        class_mode='binary')\n","\n","history = model.fit_generator(train_generator,\n","                              steps_per_epoch=100,\n","                              epochs=100,\n","                              validation_data=validation_generator,\n","                              validation_steps=50)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z4SjwvCnAR1Y"},"source":["model.save('cats_and_dogs_small_2.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DioQGCuLISwh"},"source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['acc'] \n","val_acc = history.history['val_acc'] \n","loss = history.history['loss'] \n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc') \n","plt.plot(epochs, val_acc, 'b', label='Validation acc') \n","plt.title('Training and validation accuracy') \n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss') \n","plt.plot(epochs, val_loss, 'b', label='Validation loss') \n","plt.title('Training and validation loss') \n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A6lzDsMfAP5W"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H4fdXO5ciazU"},"source":["## 5.3 Using a pretrained convnet"]},{"cell_type":"markdown","metadata":{"id":"N5nE2mk5DhKw"},"source":["* A common and highly effective approach of deep learning on small image datasets is to use a pretrained network.\n","  * A *pretrained network* is a saved network that was previously trained on a large dataset.\n","  * For instance, you might train a network on ImageNet (where classes are mostly animals and everyday objects) and then repurpose this trained network for identifying furniture items in images.\n","\n","* Such portability of learned features across different problems is a key advantage of deep learning compared to many other approaches.\n","\n","* Here, let's consider a large CNN trained on the ImageNet dataset (1.4M labeled images and 1,000 different classes).\n","\n","* We will use the VGG16 architecture, developed by Karen Simonyan and Andrew Zisserman in 2014."]},{"cell_type":"markdown","metadata":{"id":"GLvHPurCEnSo"},"source":["> ### Feature extraction"]},{"cell_type":"markdown","metadata":{"id":"2zYOt8INEqXm"},"source":["* Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples.\n","\n","  ><img src=\"https://drive.google.com/uc?id=16Qbe2uu4I0iR3yCRwEKnaUwLxQ7Sx-uk\" width=\"700\">\n","\n","* Why only reuse the convolutional base? Could we reuse the densely connected classifier as well?\n","\n","* Note that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. \n","  * Layers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), whereas layers that are higher up extract more-abstract concepts (such as “cat ear” or “dog eye”).\n","\n","* The VGG16 model comes prepackaged with Keras.\n","  * `keras.applications` module\n","  * Other models: Xception, Inception V3, ResNet 50, ...\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"tWn9DfSMF62a"},"source":["from tensorflow.keras.applications import VGG16\n","\n","conv_base = VGG16(weights='imagenet',\n","                  include_top=False,\n","                  input_shape=(150, 150, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qd-oY5_VGEdj"},"source":["conv_base.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NvxGuDFlIu4l"},"source":["* There are two ways we could proceed:\n","  * Option 1) Extract features using the convolutional base, and then save them on disk.\n","  * Option 2) Extend the model by adding `Dense` layers on top, and train it.\n","\n","* **Option 1)**\n","  * Extracting features using the pretrained convolutional base"]},{"cell_type":"code","metadata":{"id":"dtLlHkRJJvUS"},"source":["import os \n","import numpy as np \n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","base_dir = '/content/dogs_vs_cats_subset/subset'\n","train_dir = os.path.join(base_dir, 'train') \n","validation_dir = os.path.join(base_dir, 'validation') \n","test_dir = os.path.join(base_dir, 'test')\n","\n","datagen = ImageDataGenerator(rescale=1./255) \n","batch_size = 20\n","\n","def extract_features(directory, sample_count):\n","  features = np.zeros(shape=(sample_count, 4, 4, 512))\n","  labels = np.zeros(shape=(sample_count))\n","  generator = datagen.flow_from_directory(directory,\n","                                          target_size=(150, 150),\n","                                          batch_size=batch_size,\n","                                          class_mode='binary')\n","  i=0\n","  for inputs_batch, labels_batch in generator:\n","    features_batch = conv_base.predict(inputs_batch)\n","    features[i*batch_size: (i+1)*batch_size] = features_batch\n","    labels[i*batch_size: (i+1)*batch_size] = labels_batch\n","    i += 1\n","    if i*batch_size >= sample_count:\n","      break\n","  return features, labels\n","\n","train_features, train_labels = extract_features(train_dir, 2000)\n","validation_features, validation_labels = extract_features(validation_dir, 1000)\n","test_features, test_labels = extract_features(test_dir, 1000)\n","\n","# reshape\n","train_features = np.reshape(train_features, (2000, 4*4*512))\n","validation_features = np.reshape(validation_features, (1000, 4*4*512))\n","test_features = np.reshape(test_features, (1000, 4*4*512))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qmIskVHYLIFt"},"source":["from tensorflow.keras import models\n","from tensorflow.keras import layers\n","from tensorflow.keras import optimizers\n","\n","model = models.Sequential() \n","model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512)) \n","model.add(layers.Dropout(0.5)) \n","model.add(layers.Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer=optimizers.RMSprop(lr=2e-5), \n","              loss='binary_crossentropy', \n","              metrics=['acc'])\n","\n","history = model.fit(train_features, \n","                    train_labels,\n","                    epochs=30, \n","                    batch_size=20, \n","                    validation_data=(validation_features, validation_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TPwWFGa0Lqnf"},"source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['acc'] \n","val_acc = history.history['val_acc'] \n","loss = history.history['loss'] \n","val_loss = history.history['val_loss']\n","\n","epochs = range(1, len(acc) + 1)\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc') \n","plt.plot(epochs, val_acc, 'b', label='Validation acc') \n","plt.title('Training and validation accuracy') \n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'bo', label='Training loss') \n","plt.plot(epochs, val_loss, 'b', label='Validation loss') \n","plt.title('Training and validation loss') \n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wz8UZOhQL6wW"},"source":["* **Option 2)**\n","  * Extending the `conv_base` model and running it end to end on the inputs"]},{"cell_type":"code","metadata":{"id":"1SWy_nklMP60"},"source":["from tensorflow.keras import models\n","from tensorflow.keras import layers\n","\n","model = models.Sequential()\n","model.add(conv_base)\n","model.add(layers.Flatten())\n","model.add(layers.Dense(256, activation='relu'))\n","model.add(layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uqtdsBkRMbDK"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RLgSHZK8Meix"},"source":["* Before you compile and train the model, it is very important to freeze the convolutional base.\n","  * *Freezing* a layer or a set of layers means preventing their weights from being updated during training.\n","\n","* In Keras, you freeze a network by setting its `trainable` attribute to `False`."]},{"cell_type":"code","metadata":{"id":"W8bZFMC0M9_I"},"source":["print('This is the number of trainable weights ' \n","      'before freezing the conv base:', len(model.trainable_weights))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hbd7mmnNNHTR"},"source":["conv_base.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLhDaw8INK0N"},"source":["print('This is the number of trainable weights ' \n","      'after freezing the conv base:', len(model.trainable_weights))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpTr4DfTNMq1"},"source":["train_datagen = ImageDataGenerator(rescale=1./255,\n","                                   rotation_range=40,\n","                                   width_shift_range=0.2,\n","                                   height_shift_range=0.2,\n","                                   shear_range=0.2,\n","                                   zoom_range=0.2,\n","                                   horizontal_flip=True,\n","                                   fill_mode='nearest')\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(train_dir,\n","                                                    target_size=(150,150),\n","                                                    batch_size=20,\n","                                                    class_mode='binary')\n","\n","validation_generator = test_datagen.flow_from_directory(validation_dir,\n","                                                        target_size=(150,150),\n","                                                        batch_size=20,\n","                                                        class_mode='binary')\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=1e-4),\n","              metrics=['acc'])\n","\n","history = model.fit_generator(train_generator,\n","                              steps_per_epoch=100,\n","                              epochs=30,\n","                              validation_data=validation_generator,\n","                              validation_steps=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HtEb78QjNxVn"},"source":["> ### Fine-tuning"]},{"cell_type":"markdown","metadata":{"id":"wJHEG5oxR0w2"},"source":["* Another widely used technique for model reuse is *fine-tuning*.\n","  * Unfreezing a few of the top layers of a frozen model base, and jointly training both the newly added part of the model and these top layers.\n","\n","    ><img src=\"https://drive.google.com/uc?id=16YCVUxDsZ4Qlt5A05jjizSIcOPKVjjJP\" width=\"700\">\n","  \n","* The steps for fine-tuning a network\n","  * Add the custom network on top of an already-trained base network.\n","  * Freeze the base network.\n","  * Train the part you added.\n","  * Unfreeze some layers in the base network.\n","  * Jointly train both these layers and the part you added."]},{"cell_type":"code","metadata":{"id":"eoFdUbJWS3eI"},"source":["conv_base.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NpLA4TWlS7J0"},"source":["* We will fine-tune the last three convolutional layers.\n","  * All layers up to `block4_pool` should be frozen.\n","  \n","* Why not fine-tune more layers? Why not fine-tune the entire convolutional base?"]},{"cell_type":"code","metadata":{"id":"0Q4ceanOTTf6"},"source":["conv_base.trainable = True\n","\n","set_trainable = False\n","for layer in conv_base.layers:\n","  if layer.name == 'block5_conv1':\n","    set_trainable = True\n","  if set_trainable:\n","    layer.trainable = True\n","  else:\n","    layer.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuHDxOGsTlt3"},"source":["model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=1e-5), # very low learning rate\n","              metrics=['acc'])\n","\n","history = model.fit_generator(train_generator, \n","                              steps_per_epoch=100, \n","                              epochs=100, \n","                              validation_data=validation_generator, \n","                              validation_steps=50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qixeQxs4jzWE"},"source":["* **Exercise**\n","  * Evaluate the final model on the test data. "]},{"cell_type":"code","metadata":{"id":"K33sK3q-lfwi"},"source":[""],"execution_count":null,"outputs":[]}]}